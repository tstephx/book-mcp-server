# Unified Book Processing Architecture

**Status:** Implemented
**Date:** 2025-01-30
**Commits:**
- `book-ingestion-python`: `85d198e` - refactor: restructure as importable library
- `book-mcp-server`: `8189b23` - feat: integrate book-ingestion library

---

## Executive Summary

We restructured `book-ingestion-python` as an importable Python library and integrated it directly into `agentic_pipeline`. This eliminates subprocess calls, provides better error handling, and enables LLM-assisted chapter detection for low-confidence books.

### Key Benefits

| Before | After |
|--------|-------|
| Subprocess calls to CLI | Direct library imports |
| String parsing of stdout | Typed PipelineResult objects |
| No quality metrics | Access to quality_score, confidence, warnings |
| Manual error handling | Python exceptions with stack traces |
| No LLM fallback | Automatic LLM assistance for low-confidence detection |

---

## What Was Built

### 1. book-ingestion-python as Library

```
book-ingestion-python/
├── book_ingestion/           # Renamed from src/
│   ├── __init__.py           # Public API with lazy loading
│   ├── __main__.py           # python -m book_ingestion
│   ├── bootstrap.py          # Composition root (BookIngestionApp)
│   ├── ports/                # Protocol interfaces
│   │   ├── llm_fallback.py   # LLMFallbackPort
│   │   ├── repository.py     # BookRepository
│   │   └── logger.py         # PipelineLogger
│   ├── embeddings/           # Embedding generation
│   │   └── generator.py      # EmbeddingGenerator (lazy torch)
│   ├── processors/           # Core processing logic
│   ├── converters/           # PDF/EPUB converters
│   ├── storage/              # Database and file writers
│   └── utils/                # Config and helpers
├── pyproject.toml            # Package configuration
└── tests/                    # Updated test imports
```

### 2. agentic_pipeline Integration

```
agentic_pipeline/
├── adapters/                 # NEW: Integration layer
│   ├── __init__.py
│   ├── processing_adapter.py # Wraps BookIngestionApp
│   └── llm_fallback_adapter.py  # Implements LLMFallbackPort
└── orchestrator/
    └── orchestrator.py       # UPDATED: Uses ProcessingAdapter
```

---

## How It Works

### Before (Subprocess)

```python
# Old approach - orchestrator.py
def _run_processing(self, book_path: str):
    result = subprocess.run(
        [python_cmd, "-m", "src.cli", "process", book_path],
        cwd=str(self.config.book_ingestion_path),
        capture_output=True
    )
    if result.returncode != 0:
        raise ProcessingError(result.stderr)
```

### After (Direct Import)

```python
# New approach - orchestrator.py
def _run_processing(self, book_path: str, book_id: str = None):
    result = self.processing_adapter.process_book(
        book_path=book_path,
        book_id=book_id,
    )
    if not result.success:
        raise ProcessingError(result.error)

    return {
        "book_id": result.book_id,
        "quality_score": result.quality_score,
        "detection_confidence": result.detection_confidence,
        "needs_review": result.needs_review,
        "warnings": result.warnings,
        "chapter_count": result.chapter_count,
    }
```

### LLM Fallback Flow

```
┌─────────────────────┐
│ Process Book        │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│ Chapter Detection   │
└──────────┬──────────┘
           │
           ▼
     confidence < 0.5?
           │
     ┌─────┴─────┐
     │ Yes       │ No
     ▼           ▼
┌─────────┐  ┌──────────┐
│ LLM     │  │ Continue │
│ Fallback│  │ Normal   │
└────┬────┘  └──────────┘
     │
     ▼
┌─────────────────────┐
│ Apply Improvements  │
│ - Merge suggestions │
│ - Split suggestions │
│ - Confidence boost  │
└─────────────────────┘
```

---

## Verification Checklist

Run these commands to verify the implementation:

### 1. Package Installation

```bash
# In book-ingestion-python
cd /path/to/book-ingestion-python
./venv/bin/pip install -e .

# Verify import
./venv/bin/python -c "from book_ingestion import EnhancedPipeline; print('OK')"
```

### 2. CLI Works

```bash
# Via module
./venv/bin/python -m book_ingestion --help

# Via entry point
./venv/bin/book-ingestion --help
```

### 3. Tests Pass

```bash
./venv/bin/pytest tests/ -v
# Expected: 184 passed, 1 failed (semantic chunker needs sentence-transformers)
```

### 4. Integration Test

```bash
# In book-mcp-server
cd /path/to/book-mcp-server
.venv/bin/pip install -e /path/to/book-ingestion-python

.venv/bin/python -c "
from agentic_pipeline.adapters.processing_adapter import ProcessingAdapter
from pathlib import Path
adapter = ProcessingAdapter(db_path=Path('/tmp/test.db'))
print('Integration OK')
"
```

---

## Stakeholder Review Guide

### For Engineering Review

**Focus Areas:**

1. **Architecture Pattern**
   - Ports/Adapters pattern for dependency injection
   - Lazy loading prevents heavy imports at startup
   - Protocol classes define contracts between systems

2. **Code Changes**
   - `book_ingestion/__init__.py:1-100` - Public API design
   - `book_ingestion/bootstrap.py:1-250` - Composition root
   - `agentic_pipeline/adapters/processing_adapter.py` - Integration layer

3. **Test Coverage**
   - 184/185 tests passing
   - Tests updated for new import paths
   - Integration verified manually

**Questions to Discuss:**
- Is lazy loading sufficient for heavy dependencies?
- Should we add more Protocol implementations?
- How to handle embedding generation timeouts?

### For Product Review

**User-Facing Changes:**

| Feature | Impact |
|---------|--------|
| CLI still works | No change to `book-ingestion process book.pdf` |
| Better error messages | More helpful when processing fails |
| Quality metrics | Orchestrator now has access to quality_score |
| LLM assistance | Low-confidence books get automatic help |

**Demo Flow:**

1. Process a well-structured book → High confidence, auto-approved
2. Process a poorly-structured book → Low confidence, LLM fallback triggered
3. Show quality metrics in pipeline record

### For Operations Review

**Deployment Changes:**

1. **Dependencies**
   - `book-ingestion-python` must be pip-installable
   - No more venv path dependency in orchestrator

2. **Configuration**
   - Removed: `BOOK_INGESTION_PATH` environment variable (still works as fallback)
   - Added: Optional `LLM_FALLBACK_THRESHOLD` (default: 0.5)

3. **Monitoring**
   - New metrics available: `processing_result.quality_score`, `detection_confidence`
   - New flag: `llm_fallback_used` in pipeline records

**Rollback Plan:**
- Git revert both commits
- Reinstall old version of book-ingestion-python
- Subprocess calls are still compatible

---

## Architecture Decisions

### ADR-1: Ports/Adapters Pattern

**Context:** Need to inject LLM fallback without coupling book-ingestion to specific LLM providers.

**Decision:** Use Protocol classes (ports) that adapters implement.

**Consequences:**
- (+) book-ingestion is provider-agnostic
- (+) Easy to test with mock implementations
- (-) More files and indirection

### ADR-2: Lazy Loading

**Context:** torch and sentence-transformers are heavy imports (~2s startup).

**Decision:** Use `__getattr__` for lazy module loading.

**Consequences:**
- (+) Fast imports when embeddings not needed
- (+) Core functionality works without optional deps
- (-) IDE autocomplete may not work for lazy-loaded classes

### ADR-3: Direct Library Integration

**Context:** Subprocess calls lose structured data and have poor error handling.

**Decision:** Replace subprocess with direct library imports via ProcessingAdapter.

**Consequences:**
- (+) Access to typed PipelineResult with all metrics
- (+) Better stack traces on errors
- (+) No stdout/stderr parsing
- (-) Tighter coupling (mitigated by adapter pattern)

---

## Files Changed

### book-ingestion-python

| File | Change |
|------|--------|
| `src/` → `book_ingestion/` | Renamed directory |
| `book_ingestion/__init__.py` | New: Public API |
| `book_ingestion/__main__.py` | New: Module entry point |
| `book_ingestion/bootstrap.py` | New: BookIngestionApp |
| `book_ingestion/ports/` | New: Protocol interfaces |
| `book_ingestion/embeddings/` | New: EmbeddingGenerator |
| `pyproject.toml` | New: Package config |
| `book_ingestion/cli.py` | Updated imports |
| `tests/**` | Updated imports |

### book-mcp-server

| File | Change |
|------|--------|
| `agentic_pipeline/adapters/` | New: Integration layer |
| `agentic_pipeline/orchestrator/orchestrator.py` | Uses ProcessingAdapter |
| `agentic_pipeline/db/pipelines.py` | Added update_processing_result |
| `pyproject.toml` | Added book-ingestion dependency |

---

## Next Steps

1. **Short-term**
   - Install sentence-transformers to fix remaining test
   - Add integration tests for LLM fallback
   - Document ProcessingAdapter API

2. **Medium-term**
   - Add metrics/telemetry for processing times
   - Implement actual LLM calls in LLMFallbackAdapter
   - Add retry logic with exponential backoff

3. **Long-term**
   - Consider async processing for large batches
   - Add streaming support for large books
   - Implement distributed processing
